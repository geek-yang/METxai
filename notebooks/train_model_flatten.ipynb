{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arctic sea ice forecasting using deep neural networks.\n",
    "\n",
    "A CNN models will be trained to predict the sea ice concentration (SIC, from 0 - 1) in the Barents Sea, with SIC and the change of ocean heat content (OHC) as inputs.\n",
    "\n",
    "The model will be used for XAI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to processed data\n",
    "path_data = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end time of the period of interest\n",
    "start_year = 2000\n",
    "end_year = 2017\n",
    "# number of weeks per year (assuming each month contains 4 weeks)\n",
    "nweeks = 48\n",
    "# load preprocessed data - sea ice concentration\n",
    "sic_xr = xr.load_dataset(path_data / \"sic_erai_2000_2017.nc\")\n",
    "sic_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data - ocean heat content\n",
    "ohc_xr = xr.load_dataset(path_data /  \"ohc_oras_2000_2017.nc\")\n",
    "ohc_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dataset into sequences\n",
    "sic_seq = sic_xr.sic.values.reshape(-1, sic_xr.latitude.size, sic_xr.longitude.size)\n",
    "ohc_seq = ohc_xr.ohc.values.reshape(-1, ohc_xr.latitude.size, ohc_xr.longitude.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked area from -1 to 0\n",
    "sic_seq[sic_seq < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(x):\n",
    "    max_value = np.amax(x)\n",
    "    min_value = np.amin(x)\n",
    "    y = (x - min_value)/(max_value - min_value)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize ohc\n",
    "ohc_norm_seq = normalizer(ohc_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 years data used for training, in total 12 * 48 (weeks) data points (spatial maps)\n",
    "# we deal with timeseries, the data should not be shuffled\n",
    "train_years = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build neural network. <br>\n",
    "A simple 5 layer CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetNet(nn.Module):\n",
    "    def __init__(self, input_channels = 2, kernels=[2, 3, 5, 3], dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.kernels = kernels\n",
    "        # layer 1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, kernels[0], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # layer 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # layer 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[1], kernels[2], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # layer 4\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[2], kernels[3], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # fc layer\n",
    "        self.fc = nn.Linear(sic_xr.latitude.size * sic_xr.longitude.size * kernels[3], sic_xr.latitude.size * sic_xr.longitude.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment and choose parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "#print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "input_channels = 2\n",
    "epochs = 50\n",
    "kernels=[2, 3, 5, 3]\n",
    "learning_rate = 0.005\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "model = MetNet(input_channels, kernels, dropout)\n",
    "# choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "# check the setups\n",
    "print(model)\n",
    "print(loss_fn)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start model training\n",
    "hist = np.zeros(epochs)\n",
    "# switch model into train mode\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for timestep in range(train_years * nweeks):\n",
    "        X_train = torch.tensor(np.stack((sic_seq[timestep,:,:],\n",
    "                                        ohc_norm_seq[timestep,:,:]))\n",
    "                              ).float().view(-1, input_channels, \n",
    "                                             sic_xr.latitude.size,\n",
    "                                             sic_xr.longitude.size)\n",
    "        x_var = torch.autograd.Variable(X_train)\n",
    "        y_train = torch.tensor(sic_seq[timestep+1,:,:]).float().view(-1,\n",
    "                                                                     sic_xr.latitude.size *\n",
    "                                                                     sic_xr.longitude.size)\n",
    "        y_var = torch.autograd.Variable(y_train)\n",
    "        # forward pass\n",
    "        outputs = model(x_var)\n",
    "        # check loss\n",
    "        loss = loss_fn(outputs, y_var)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # computing gradients\n",
    "        loss.backward()\n",
    "        # accumulating running loss\n",
    "        running_loss += loss.item()\n",
    "        # updated weights based on computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:    \n",
    "        print(f'Epoch {epoch + 1}/{epochs} running accumulative loss across all batches:' + \n",
    "              f'{running_loss:.3f}')\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the general checkpoint of model in pytorch format\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, Path('./models/metnet_flatten_training_checkpoint.pt'))\n",
    "print(\"The checkpoint of the model and training status is saved.\")\n",
    "\n",
    "# export to onnx format\n",
    "onnx_file = Path('./models/metnet_flatten.onnx')\n",
    "# Export the model\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x_var,                     # model input (or a tuple for multiple inputs)\n",
    "                  onnx_file,                 # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "print(\"The model is exported to onnx format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the time of interest in testing set\n",
    "week_interest = 5\n",
    "year_interest = 2012\n",
    "timestep_test = train_years * nweeks + (year_interest - 2012) * nweeks + week_interest\n",
    "# evalutation mode\n",
    "model.eval()\n",
    "X_test = torch.tensor(np.stack((sic_seq[timestep_test,:,:],\n",
    "                                ohc_norm_seq[timestep_test,:,:]))\n",
    "                        ).float().view(-1, input_channels, \n",
    "                                        sic_xr.latitude.size,\n",
    "                                        sic_xr.longitude.size)\n",
    "x_var = torch.autograd.Variable(X_test, requires_grad=False)\n",
    "pred = model(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred[0,0,:,:].data.numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sic_seq[timestep_test+1,:,:]) # plus one because model predicts one timestep ahead\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "951e587de391aa2bb289e8fbd39b65d4ffaa4789dc01c18d4fc05216cb0e7d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
