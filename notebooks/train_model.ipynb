{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arctic sea ice forecasting using deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "path_data = Path(\"/home/yangliu/MLexpo/data\")\n",
    "erai_path = path_data / \"ERA-Interim\"\n",
    "oras_path = path_data / \"ORAS4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2000\n",
    "end_year = 2017\n",
    "# number of weeks per year\n",
    "nweeks = 48\n",
    "# load data\n",
    "sic_xr = xr.load_dataset(erai_path / \"sic_weekly_erai_1979_2017.nc\")\n",
    "sic_xr = sic_xr.sel(year=slice(start_year, end_year), latitude=slice(81, 63),\n",
    " longitude=slice(18, 59.5))\n",
    "sic_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask from ORAS4 data\n",
    "mask_ocean = xr.where(sic_xr['sic'][0,0,:,:] < 0, 1, 0)\n",
    "mask_ocean.rename('mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc_xr = xr.load_dataset(oras_path / \"ohc_monthly_oras2erai_1978_2017.nc\")\n",
    "# the coordinate name is wrong for week, it should be month\n",
    "ohc_xr = ohc_xr.rename({'week': 'month'})\n",
    "# 1 more year inlucded to calculate the relative difference\n",
    "ohc_xr = ohc_xr.sel(year=slice(start_year - 1, end_year), latitude=slice(81, 63),\n",
    " longitude=slice(18, 59.5))\n",
    "ohc_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dataset into sequences\n",
    "sic_seq = sic_xr.sic.values.reshape(-1, sic_xr.latitude.size, sic_xr.longitude.size)\n",
    "ohc_seq_monthly = ohc_xr.OHC.values.reshape(-1, ohc_xr.latitude.size, ohc_xr.longitude.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked area from -1 to 0\n",
    "sic_seq[sic_seq < 0] = 0\n",
    "# interpolation of ohc from monthly to weekly\n",
    "ohc_diff_weekly = (ohc_seq_monthly[1:,:,:] - ohc_seq_monthly[:-1,:,:]) / 4\n",
    "ohc_seq = np.zeros((sic_xr.year.size * nweeks, sic_xr.latitude.size, sic_xr.longitude.size),dtype=float)\n",
    "# calculate relative difference, note that the first year data is edge case\n",
    "for i in range(4):\n",
    "    ohc_seq[3-i::4,:,:] = ohc_seq_monthly[12:,:,:] - i * ohc_diff_weekly[11:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(x):\n",
    "    max_value = np.amax(x)\n",
    "    min_value = np.amin(x)\n",
    "    y = (x - min_value)/(max_value - min_value)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize ohc\n",
    "ohc_norm_seq = normalizer(ohc_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we deal with timeseries, the data should not be shuffled\n",
    "train_years = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetNet(nn.Module):\n",
    "    def __init__(self, input_channels = 2, kernels=[2, 4, 1], dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.kernels = kernels\n",
    "        # layer 1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, kernels[0], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # layer 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # layer 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[1], kernels[2], kernel_size=5, stride = 1, padding = 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "#print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "input_channels = 2\n",
    "epochs = 30\n",
    "kernels = [2, 4, 1]\n",
    "learning_rate = 0.01\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "model = MetNet(input_channels, kernels, dropout)\n",
    "# choose optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "# check the setups\n",
    "print(model)\n",
    "print(loss_fn)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.zeros(epochs)\n",
    "# switch model into train mode\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for timestep in range(train_years * nweeks):\n",
    "        X_train = torch.tensor(np.stack((sic_seq[timestep,:,:],\n",
    "                                        ohc_norm_seq[timestep,:,:]))\n",
    "                              ).float().view(-1, input_channels, \n",
    "                                             sic_xr.latitude.size,\n",
    "                                             sic_xr.longitude.size)\n",
    "        x_var = torch.autograd.Variable(X_train)\n",
    "        y_train = torch.tensor(sic_seq[timestep+1,:,:]).float().view(-1, 1, \n",
    "                                                                     sic_xr.latitude.size,\n",
    "                                                                     sic_xr.longitude.size)\n",
    "        y_var = torch.autograd.Variable(y_train)\n",
    "        # forward pass\n",
    "        outputs = model(x_var)\n",
    "        # check loss\n",
    "        loss = loss_fn(outputs, y_var)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # computing gradients\n",
    "        loss.backward()\n",
    "        # accumulating running loss\n",
    "        running_loss += loss.item()\n",
    "        # updated weights based on computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:    \n",
    "        print(f'Epoch {epoch + 1}/{epochs} running accumulative loss across all batches:' + \n",
    "              f'{running_loss:.3f}')\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the general checkpoint\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item()\n",
    "            }, Path('../xai/models/metnet_training_checkpoint.pt'))\n",
    "print(\"The checkpoint of the model and training status is saved.\")\n",
    "\n",
    "# export to onnx format\n",
    "onnx_file = Path('../xai/models/metnet.onnx')\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x_var,                         # model input (or a tuple for multiple inputs)\n",
    "                  onnx_file,                 # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "print(\"The model is exported to onnx format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_test = train_years * nweeks + 5\n",
    "# evalutation mode\n",
    "model.eval()\n",
    "X_test = torch.tensor(np.stack((sic_seq[timestep_test,:,:],\n",
    "                                ohc_norm_seq[timestep_test,:,:]))\n",
    "                        ).float().view(-1, input_channels, \n",
    "                                        sic_xr.latitude.size,\n",
    "                                        sic_xr.longitude.size)\n",
    "x_var = torch.autograd.Variable(X_test, requires_grad=False)\n",
    "pred = model(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred[0,0,:,:].data.numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sic_seq[timestep_test+1,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "dl = DeepLift(model)\n",
    "gs = GradientShap(model)\n",
    "\n",
    "ig_attr_test = ig.attribute(X_test, n_steps=10)\n",
    "dl_attr_test = dl.attribute(X_test)\n",
    "gs_attr_test = gs.attribute(X_test, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "951e587de391aa2bb289e8fbd39b65d4ffaa4789dc01c18d4fc05216cb0e7d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
